career[ , qvalue := cumsum(PEP) / 1:nrow(career) ]
career[ qvalue < .05, ]
library(ggplot2)#
library(data.table)#
#
# set float numbers to print only three digits after the decimal point#
options( digits = 3 )#
#
# -----------------------------------------------------------------------#
# beta distribution#
# http://varianceexplained.org/statistics/beta_distribution_and_baseball/#
# -----------------------------------------------------------------------#
#
# simulated data,#
# generate a sequence of numbers for each combination of a and b#
# to plot the probability density function.#
# "\u03B1" unicode for the greek letter alpha#
sim <- data.table( a = c( 81, 82, 81 + 100 ),#
                   b = c( 219, 219, 219 + 200 ) )#
sim <- sim[ , .( x = seq( 0, 1, by = 0.002 ) ), by = .( a, b ) ]#
#
sim[ , `:=`( y = dbeta( x, a, b ),#
             parameters = paste0( "\u03B1 = ", a, ", \u03B2 = ", b ) ) ]#
sim[ , parameters := factor( parameters, levels = unique(parameters) ) ]#
# plot of the distribution#
PlotBeta <- function(sim)#
{#
    ggplot( sim, aes( x, y, color = parameters ) ) + geom_line() +#
    xlim( 0, .5 ) + ylab("Density of beta") + theme_bw()#
}#
PlotBeta( sim = sim[ a == 81, ] )#
#
# update 1 hit of 1 bat#
PlotBeta( sim = sim[ a %in% c( 81, 82 ), ] )#
#
# update 100 hit of 300 bat#
PlotBeta( sim = sim )#
# -----------------------------------------------------------------------#
# empirical bayes#
# http://varianceexplained.org/r/empirical_bayes_baseball/#
# -----------------------------------------------------------------------#
#
# load the batting and pitching data from the Lahman package#
# the Master is used to get further details e.g. corresponding #
# player name for the player id column in the Batting and Pitching data#
library(Lahman)#
data(Master)#
data(Batting)#
data(Pitching)#
#
master   <- data.table( Master  , key = "playerID" )#
pitching <- data.table( Pitching, key = "playerID" )#
batting  <- data.table( Batting , key = "playerID" )#
#
# ! stands for not join,#
# return all rows from x, where there're no matching values in y#
# H = hits#
# AB = at bats#
career <- batting[ AB > 0, ][!pitching]#
career <- career[ , .( H = sum(H), AB = sum(AB) ), by = playerID ]#
career[ , average := H / AB ]#
#
# map the player name to player id#
master <- master[ , .( playerID, nameFirst, nameLast ) ]
master[ , name := paste( nameFirst, nameLast ) ][career]
master[ , name := paste( nameFirst, nameLast ) ][career, nomatch = 0]
library(dplyr)#
library(gapminder)
gapminder
gapminder <- gapminder %>% mutate( year1950 = year - 1950 )
gapminder
nest
nest()
library(tidyr)
nest
select_vars
?select_vars
nest_
class(gapminder)
nest_.tbl_df
nest_.data.frame
col_name
?col_name
gapminder %>%#
group_by( continent, country ) %>%#
nest()
by_country <- gapminder %>%#
              group_by( continent, country ) %>%#
              nest()#
#
country_model <- function(df){#
    lm( lifeExp ~ year1950, data = df )#
}#
models <- by_country %>%#
          mutate( mod = map( data, country_model ) )
library(purrr)
by_country <- gapminder %>%#
              group_by( continent, country ) %>%#
              nest()#
#
country_model <- function(df){#
    lm( lifeExp ~ year1950, data = df )#
}#
models <- by_country %>%#
          mutate( mod = map( data, country_model ) )
models
mtcars
map_dbl( mtcars, mean )
map_dbl
mtcars[[1]]
models
gapminder <- gapminder %>% mutate( year1950 = year - 1950 )#
by_country <- gapminder %>%#
              group_by( continent, country ) %>%#
              nest()#
#
country_model <- function(df){#
    lm( lifeExp ~ year1950, data = df )#
}#
models <- by_country %>%#
          mutate( mod = map( data, country_model ) )
library(tidyr)#
library(dplyr)#
library(purrr)#
library(gapminder)#
# number of years since 1950#
gapminder <- gapminder %>% mutate( year1950 = year - 1950 )#
by_country <- gapminder %>%#
              group_by( continent, country ) %>%#
              nest()#
#
country_model <- function(df){#
    lm( lifeExp ~ year1950, data = df )#
}#
models <- by_country %>%#
          mutate( mod = map( data, country_model ) )
models
models %>% filter( continent == "Africa" )
library(broom)
models <- models %>% #
          mutate( tidy = map( model, broom::tidy ),#
                  glance = map( model, broom::glance ),#
                  augment = map( model, broom::augment ) )
?qbeta
models <- models %>% #
          mutate( tidy = map( models, broom::tidy ),#
                  glance = map( models, broom::glance ),#
                  augment = map( models, broom::augment ) )
modela
models
models$data
class(models$data)
models
models$mod[[1]]
tidy(models$mod[[1]])
glance(models$mod[[1]])#
augment(models$mod[[1]])
models <- models %>% #
          mutate( tidy = map( mod, broom::tidy ),#
                  glance = map( mod, broom::glance ),#
                  augment = map( mod, broom::augment ) )
models
models <- by_country %>%#
          mutate( mod = map( data, country_model ) )
by_country
models <- models %>% #
          mutate( tidy = map( mod, broom::tidy ),#
                  glance = map( mod, broom::glance ),#
                  augment = map( mod, broom::augment ),#
                  rsq = glance %>% map_dbl("r.squared") )
models
unnest( models, data )
models
library(ggplot2) # devtools::install_github("hadley/ggplot2") or subtitles won't work#
library(tidyr)#
library(dplyr)#
library(readr)#
library(scales)
file <- "nytimes_vote.tsv"#
if( !file.exists(file) ){#
    url <- "https://static01.nyt.com/newsgraphics/2016/04/21/undervote/ad8bd3e44231c1091e75621b9f27fe31d116999f/data.tsv"#
    download.file( url, file )#
}
df <- read_tsv(file)
library(readr)
df <- read_tsv(file)
df
rename(df, `Someone else`=undervt, `Hilary Clinton`=clintonpct, `Bernie Sanders`=sanderspct)
?rename
df <- df %>% rename( "Someone else" = undervt, #
                     "Hilary Clinton" = clintonpct, #
                     "Bernie Sanders" = sanderspct )
df
select(df, -tvotes)
gather(select(df, -tvotes), party, pct, -ratio, -fips)
?gather
df <- gather( select( df, -tvotes ), party, pct, -ratio, -fips )#
df <- arrange( df, fips, ratio )#
df <- df %>% mutate( #
    party = factor( party, levels = c("Hilary Clinton", "Bernie Sanders", "Someone else" ) )#
)#
df
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point()
ggplot( df, aes( x = ratio, y = pct, color = party ) ) + #
geom_point()
ggplot( df, aes( x = ratio, y = pct, color = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white" )
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white" )
ggplot( df, aes( x = ratio, y = pct, color = party ) ) + #
geom_point( size = 3, alpha = 0.8, fill = "white" )
ggplot( df, aes( x = ratio, y = pct, color = party ) ) + #
geom_point( size = 3, alpha = 0.8, fill = "white", shape = 21 )
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, shape = 21 )
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, fill = "white", shape = 21 )
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white", shape = 21 )
ggplot( df, aes( x = ratio, y = pct ) ) + #
geom_point( size = 3, alpha = 0.8, shape = 21 )
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white", shape = 21 )
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white", shape = 21 ) + #
scale_fill_manual( name = "", values = c( "Hilary Clinton" = "#5fa0d6",#
                                          "Bernie Sanders" = "#83BC57",#
                                          "Someone else" = "#d65454" ) )
fill_color <- c( #
    "Hilary Clinton" = "#5fa0d6",#
    "Bernie Sanders" = "#83BC57",#
    "Someone else" = "#d65454" #
)#
#
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white", shape = 21 ) + #
scale_fill_manual( name = "", values = fill_color )
library(ggplot2) # devtools::install_github("hadley/ggplot2") or subtitles won't work#
library(tidyr)#
library(dplyr)#
library(readr)#
library(scales)
sessionInfo()
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white", shape = 21 ) + #
scale_fill_manual( name = "", values = fill_color ) +#
labs( x = "Ratio of registered Democrats to Obama voters →", #
      y = NULL, title = "The Kinds of Places Sanders Beats Clinton",#
      subtitle = "Each dot on this chart represents the share of a county's vote for a candidate in the 2016 Democratic primary" )
?labs
ggplot( df, aes( x = ratio, y = pct, fill = party ) ) + #
geom_point( size = 3, alpha = 0.8, color = "white", shape = 21 ) + #
scale_fill_manual( name = "", values = fill_color ) +#
labs( x = "Ratio of registered Democrats to Obama voters →", #
      y = NULL, title = "The Kinds of Places Sanders Beats Clinton",#
      subtitle = "Each dot on this chart represents the share of a county's vote for a candidate in the 2016 Democratic primary" )
library(Advertising)
library(ISLR)
Advertising
data(Advertising)
library(dplyr)
data(flights)
library(nycflights13)
data(flights)
flights
?write.csv
write.csv( flights, file = 'flights.csv', quote = FALSE, row.names = FALSE )
x <- seq( -4, 6, 0.1 )#
mean1 <- 0.00#
mean2 <- 2.47#
dat <- data.frame(x = x, y1 = dnorm(x, mean1, 1), y2 = dnorm(x, mean2, 1))#
ggplot(dat, aes(x = x)) +#
geom_line(aes(y = y1, colour = 'H0 is true'), size = 1.2) +#
geom_line(aes(y = y2, colour = 'H1 is true'), size = 1.2) +#
geom_area(aes(y = y1, x = ifelse(x > 1.65, x, NA)), fill = 'black') +#
geom_area(aes(y = y2, x = ifelse(x > 1.65, x, NA)), fill = 'blue', alpha = 0.3) +#
xlab("") + ylab("") + theme(legend.title = element_blank()) +#
scale_colour_manual(breaks = c("H0 is true", "H1 is true"), values = c("blue", "red"))
library(ggplot2)
ggplot(dat, aes(x = x)) +#
geom_line(aes(y = y1, colour = 'H0 is true'), size = 1.2) +#
geom_line(aes(y = y2, colour = 'H1 is true'), size = 1.2) +#
geom_area(aes(y = y1, x = ifelse(x > 1.65, x, NA)), fill = 'black') +#
geom_area(aes(y = y2, x = ifelse(x > 1.65, x, NA)), fill = 'blue', alpha = 0.3) +#
xlab("") + ylab("") + theme(legend.title = element_blank()) +#
scale_colour_manual(breaks = c("H0 is true", "H1 is true"), values = c("blue", "red"))
2500 / 0.8
2500 * 0.8
size <- 2500#
a_count <- 2500 * 0.08#
b_count <- 2500 * 0.1#
#
pooled <- ( a_count + b_count ) / ( size + size )#
0.02 / sqrt( pooled * ( 1 - pooled ) * ( 1 / size ) * ( 1 / size ) )
sqrt( pooled * ( 1 - pooled ) * ( 1 / size ) * ( 1 / size ) )
pooled
pooled <- ( a_count + b_count ) / ( size + size )#
0.02 / sqrt( pooled * ( 1 - pooled ) * ( 1 / size ) + ( 1 / size ) )
0.02 / sqrt( pooled * ( 1 - pooled ) * ( 1 / size + 1 / size ) )
pnorm(Z)
Z <- ( p_b - p_a ) / sqrt( p_pooled * ( 1 - p_pooled ) * ( 1 / size + 1 / size ) )#
Z
size <- 2500#
p_a  <- 0.08#
p_b  <- 0.1#
count_a  <- 2500 * p_a#
count_b  <- 2500 * p_b#
p_pooled <- ( count_a  + count_b ) / ( size + size )#
Z <- ( p_b - p_a ) / sqrt( p_pooled * ( 1 - p_pooled ) * ( 1 / size + 1 / size ) )#
Z
pnorm(Z)
PlotPower <- function( size, min_diff ){#
    size_a <- size_b <- size * 0.5 # size are assumed to be equal#
    p_a <- 0.08 # baseline#
    p_b <- p_a + min_diff#
    count_a  <- size_a * p_a#
    count_b  <- size_b * p_b#
    p_pooled <- ( count_a  + count_b ) / ( size_a + size_b )#
    Z <- ( p_b - p_a ) / sqrt( p_pooled * ( 1 - p_pooled ) * ( 1 / size_a + 1 / size_b ) )#
#
    # Z corresponds to the mean of the normal distribution#
    mean1 <- 0#
    mean2 <- Z#
#
    x <- seq( -4, 6, 0.1 ) # use for generating the x axis of the normal distribution#
    data <- data.frame( x = x, y1 = dnorm( x, mean1, 1 ) , y2 = dnorm( x, mean2, 1 ) )#
#
    plot <- ggplot( data, aes( x = x ) ) +#
            geom_line( aes( y = y1, colour = 'H0 is true' ), size = 1.2 ) +#
            geom_line( aes( y = y2, colour = 'H1 is true' ), size = 1.2 ) +#
            geom_area( aes( y = y1, x = ifelse( x > 1.65, x, NA ) ), fill = 'black' ) +#
            geom_area( aes( y = y2, x = ifelse( x > 1.65, x, NA ) ), fill = 'blue', alpha = 0.3 ) +#
            labs( x = '', y = '', title = sprintf( 'p1 = %s, p2 = %s, size = %d', p_a, p_b, size ) ) + #
            theme( legend.title = element_blank() ) +#
            scale_colour_manual( breaks = c( "H0 is true", "H1 is true" ), #
                                 values = c( "blue", "red" ) )#
    return(plot)#
}#
#
PlotPower( size = 5000, min_diff = 0.02 )
library(ggplot2)
baseline  <- 0.1  # baseline conversion rate #
delta     <- 0.02 # minimum detectable boundary (practical significance boundary)#
power     <- 0.8  # sensitivity #
sig_level <- 0.05 # specificity #
#
result <- power.prop.test( p1 = baseline, p2 = baseline + delta, #
                           power = power, sig.level = sig_level,#
                           alternative = "two.sided" )#
round(result$n)
result <- power.prop.test( p1 = baseline, p2 = baseline - delta, #
                           power = power, sig.level = sig_level,#
                           alternative = "two.sided" )#
round(result$n)
?power.prop.test
baseline  <- 0.1 # baseline rate#
power     <- 0.8  # sensitivity #
sig_level <- 0.05#
dd <- seq( from = 0.01, to = 0.03, by = 0.0001 )
dd
result <- power.prop.test( p1 = baseline, p2 = baseline - dd, #
                           power = power, sig.level = sig_level,#
                           alternative = "two.sided" )
result <- data.table( matrix( nrow = length(dd), ncol = 2 ) )
result <- data.frame( matrix( nrow = length(dd), ncol = 2 ) )
result
library(data.table)
?data.table
result <- data.table( matrix( nrow = length(dd), ncol = 2 ) )
result
result <- matrix( nrow = length(dd), ncol = 2 )#
result[ , 1 ] <- dd
result
for( i in 1:length(dd) ){#
    result[ i, 2 ] <- power.prop.test( p1 = baseline, p2 = baseline - dd[i], #
                                       power = power, sig.level = sig_level,#
                                       alternative = "two.sided" )#
}
baseline  <- 0.1 # baseline rate#
power     <- 0.8  # sensitivity #
sig_level <- 0.05#
dd <- seq( from = 0.01, to = 0.03, by = 0.0001 ) # detectable differences#
result <- matrix( nrow = length(dd), ncol = 2 )#
result[ , 1 ] <- dd#
for( i in 1:length(dd) ){#
    result[ i, 2 ] <- power.prop.test( p1 = baseline, p2 = baseline - dd[i], #
                                       power = power, sig.level = sig_level,#
                                       alternative = "two.sided" )#
}
baseline  <- 0.1 # baseline rate#
power     <- 0.8  # sensitivity #
sig_level <- 0.05#
dd <- seq( from = 0.01, to = 0.03, by = 0.0001 ) # detectable differences#
result <- matrix( nrow = length(dd), ncol = 2 )#
result[ , 1 ] <- dd#
for( i in 1:length(dd) ){#
    result[ i, 2 ] <- power.prop.test( p1 = baseline, p2 = baseline - dd[i], #
                                       power = power, sig.level = sig_level,#
                                       alternative = "two.sided" )$n#
}
result
result <- data.table(result)#
setnames( result, c( 'dd', 'sample_size' ) )
result
library(scales)
baseline  <- 0.1 # baseline rate#
power     <- 0.8  # sensitivity #
sig_level <- 0.05#
dd <- seq( from = 0.01, to = 0.03, by = 0.0001 ) # detectable differences#
result <- matrix( nrow = length(dd), ncol = 2 )#
result[ , 1 ] <- dd#
for( i in 1:length(dd) ){#
    result[ i, 2 ] <- power.prop.test( p1 = baseline, p2 = baseline - dd[i], #
                                       power = power, sig.level = sig_level,#
                                       alternative = "two.sided" )$n#
}#
#
result <- data.table(result)#
setnames( result, c( 'dd', 'n' ) )#
#
ggplot(data = result, aes( x = dd, y = n ) ) +#
geom_line() + ylab("required sample size") + xlab("Detectable difference")
pnorm(0.95)
pnorm(95)
?pnorm
qnorm( 0.95 )
qnorm( 0.75 )
qnorm( 0.975 )
iris_data <- iris[ , -5 ]
iris_data
iris_data <- iris[ , -5 ]
iris_data <- sapply( iris_data, function(x){#
        ( x - min(x) ) / ( max(x) - min(x) )#
    }
)
iris_data
iris_data <- iris[ , -5 ]
iris_data
library(data.table)
scale(iris_data)
iris_data <- data.table( scale(iris_data) )
iris_data
data <- iris_data
data
is.matrix(data)
unique(data)
dataset <- unique(data)
unique_data <- unique(data)
nrow(unique_data)
n <- nrow(unique_data)
sample.int( n, 1 )
integer(k)
k = 3
integer(k)
center_ids <- integer(k)
center_ids[1] <- sample.int( n, 1 )
center_ids
unique_data
unique_data[ center_ids[1], ]
dists <- apply( unique_data[ center_ids[1], ], 1, function(center)#
        {#
            sqrt( rowSums( ( unique_data - center )^2 ) )#
        })
dists
?scale
scale( unqiue_data, center = center_ids[1], scale = FALSE )
scale( unique_data, center = center_ids[1], scale = FALSE )
unique_data
center <- unique_data[ center_ids[1], ]
scale( unique_data, center = center, scale = FALSE )
center
unique_data
?scale
center <- unique_data[ center_ids[1], ]#
        dists <- apply( unique_data, 1, function(datapoint)#
        {#
            sqrt( rowSums( ( datapoint - center )^2 ) )#
        })
dists
unique_data
center
d<-c( -0.535384,   0.7861738,    -1.165809,   -1.311052 ) - c( -1.13920048, -0.13153881,   -1.3357516,  -1.3110521 )
d
sum(d^2)
sqrt( sum(d^2) )
sqrt( rowSums(d^2) )
center_ids[ i + 1 ] <- sample.int( n, 1, prob = dists )
center_ids[ 1 + 1 ] <- sample.int( n, 1, prob = dists )
center_ids
?kmeans
distance <- dists
distance
center_ids[2]
center <- unique_data[ center_ids[2], ]#
        dists <- apply( unique_data, 1, function(datapoint){#
            rowSums( ( datapoint - center )^2 )#
        })
dists
distance
center
unique_data
cbind( distance, dists )
distance1 <- cbind( distance, dists )
distance1
apply( distance1, 1, min )
Kmeanspp <- function( data, k, ... )#
{#
    # kmeans++, generating a better k initial random center for kmeans. Workflow:#
    # 1. choose a data point at random from the dataset, this serves as the first center point. #
    # 2. compute the SQUARED distance of all other data points to the randomly chosen center point.#
    # 3. to generate the next center point, each data point is chosen with the prob (weight) of #
    #    its squared distance to the chosen center of this round divided by the the #
    #    total squared distance (in R, sample function's probability are already weighted, #
    #    do not need to tune them to add up to one).#
    # 4. next recompute the weight of each data point as the minimum of the distance between it and#
    #    ALL the centers that are already generated ( e.g. for the second iteration, compare the #
    #    distance of the data point between the first and second center and choose the smaller one ).#
    # 5. repeat step 3 and 4 until having k centers. #
    ##
    # Parameters#
    # ----------#
    # data : data.frame, data.table, matrix data#
    ##
    # k : int #
    #     number of clusters#
    # #
    # ... : #
    #     all other parameters that can be passed into R's kmeans except for the data and center#
    #     , see ?kmeans for more detail#
    ##
    # Returns#
    # -------#
    # result : list#
    #     R's kmeans original output#
    ##
    # Reference#
    # ---------#
    # https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/#
    if( !is.data.table(data) )#
        data <- data.table(data)#
    # used with bootstrapped data. so unique the data#
    # to avoid duplicates, or kmeans will warn about #
    # identical cluster center#
    unique_data <- unique(data)#
#
    # generate the first center randomly#
    n <- nrow(unique_data)#
    center_ids <- integer(k)#
    center_ids[1] <- sample.int( n, 1 )#
#
    for( i in 1:( k - 1 ) ){        #
        # calculate the squared distance between the center and #
        # all the data points#
        center <- unique_data[ center_ids[i], ]#
        dists <- apply( unique_data, 1, function(datapoint){#
            rowSums( ( datapoint - center )^2 )#
        })#
#
        # sample the next center using the squared distance as the weighted probability,#
        # starting from the second center, the measure "squared distance" for each data point#
        # is the min distance between each data point and each center that has already been#
        # generated#
        if( i == 1 ){       #
            distance <- dists#
#
        }else{#
            distance <- cbind( distance, dists )#
            distance <- apply( distance, 1, min )#
        }#
        center_ids[ i + 1 ] <- sample.int( n, 1, prob = distance )                  #
    }#
#
    # cluster the whole "data", using the center_ids generated using kmeanspp#
    results <- kmeans( data, centers = unique_data[ center_ids, ], ... )#
    return(results) #
}
# remove the species column#
    iris_data <- iris[ , -5 ]#
#
    # normalize the dataset#
    iris_data <- data.table( scale(iris_data) )#
    results <- kmeanspp( data = iris_data, k = 3 )
# remove the species column#
    iris_data <- iris[ , -5 ]#
#
    # normalize the dataset#
    iris_data <- data.table( scale(iris_data) )#
    results <- Kmeanspp( data = iris_data, k = 3 )
rowSums
sums
sum
unique_data
center
apply( unique_data, 1, function(datapoint){#
            rowSums( ( datapoint - center )^2 )#
        })
apply( unique_data, 1, function(datapoint){#
            sum( ( datapoint - center )^2 )#
        })
library(data.table)#
#
Kmeanspp <- function( data, k, ... )#
{#
    # kmeans++, generating a better k initial random center for kmeans. Workflow:#
    # 1. choose a data point at random from the dataset, this serves as the first center point. #
    # 2. compute the SQUARED distance of all other data points to the randomly chosen center point.#
    # 3. to generate the next center point, each data point is chosen with the prob (weight) of #
    #    its squared distance to the chosen center of this round divided by the the #
    #    total squared distance (in R, sample function's probability are already weighted, #
    #    do not need to tune them to add up to one).#
    # 4. next recompute the weight of each data point as the minimum of the distance between it and#
    #    ALL the centers that are already generated ( e.g. for the second iteration, compare the #
    #    distance of the data point between the first and second center and choose the smaller one ).#
    # 5. repeat step 3 and 4 until having k centers. #
    ##
    # Parameters#
    # ----------#
    # data : data.frame, data.table, matrix data#
    ##
    # k : int #
    #     number of clusters#
    # #
    # ... : #
    #     all other parameters that can be passed into R's kmeans except for the data and center#
    #     , see ?kmeans for more detail#
    ##
    # Returns#
    # -------#
    # result : list#
    #     R's kmeans original output#
    ##
    # Reference#
    # ---------#
    # https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/#
    if( !is.data.table(data) )#
        data <- data.table(data)#
    # used with bootstrapped data. so unique the data#
    # to avoid duplicates, or kmeans will warn about #
    # identical cluster center#
    unique_data <- unique(data)#
#
    # generate the first center randomly#
    n <- nrow(unique_data)#
    center_ids <- integer(k)#
    center_ids[1] <- sample.int( n, 1 )#
#
    for( i in 1:( k - 1 ) ){        #
        # calculate the squared distance between the center and #
        # all the data points#
        center <- unique_data[ center_ids[i], ]#
        dists <- apply( unique_data, 1, function(datapoint){#
            sum( ( datapoint - center )^2 )#
        })#
#
        # sample the next center using the squared distance as the weighted probability,#
        # starting from the second center, the measure "squared distance" for each data point#
        # is the min distance between each data point and each center that has already been#
        # generated#
        if( i == 1 ){       #
            distance <- dists#
#
        }else{#
            distance <- cbind( distance, dists )#
            distance <- apply( distance, 1, min )#
        }#
        center_ids[ i + 1 ] <- sample.int( n, 1, prob = distance )                  #
    }#
#
    # cluster the whole "data", using the center_ids generated using kmeanspp#
    results <- kmeans( data, centers = unique_data[ center_ids, ], ... )#
    return(results) #
}
"#
    # remove the species column#
    iris_data <- iris[ , -5 ]#
#
    # normalize the dataset#
    iris_data <- data.table( scale(iris_data) )#
    results <- Kmeanspp( data = iris_data, k = 3 )#
    "
# remove the species column#
    iris_data <- iris[ , -5 ]#
#
    # normalize the dataset#
    iris_data <- data.table( scale(iris_data) )#
    results <- Kmeanspp( data = iris_data, k = 3 )
results
table( iris$Species, results$cluster )
results$size
result <- kmeans( iris_data, k = 3 )
?kmeans
result <- kmeans( iris_data, centers = 3 )
table( iris$Species, result$cluster )
result$size
results$centers
result$centers
iris_data <- iris[ , -5 ]#
#
    # normalize the dataset#
    iris_data <- data.table( scale(iris_data) )#
    results <- Kmeanspp( data = iris_data, k = 3 )
results$centers
result <- kmeans( iris_data, centers = 3 )
result$centers
library(dplyr)#
library(ggplot2)#
library(data.table)#
setwd('/Users/ethen/machine-learning/sentiment/twitter_sentiment')#
# The dataset contains 14640 tweets and 15 variables (columns)#
# which we'll explore#
tweets <- fread( 'Tweets.csv', na.strings = c( '', ' ', 'NA' ) )#
dim(tweets)
apply( tweets, 2, function(x) sum( is.na(x) ) )
sentiment_freq <- data.table( prop.table( table(tweets$airline_sentiment) ) )#
setnames( sentiment_freq, c( 'sentiment', 'percentage' ) )#
#
# plot#
ggplot( sentiment_freq, aes( sentiment, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
ggtitle('Overall Sentiment Distribution') + #
theme( plot.title = element_text( size = 14, face = 'bold' ) )
ggplot( sentiment_freq, aes( sentiment, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
ggtitle('Overall Sentiment Distribution') + #
scale_color_manual( values = c( "indianred1", "deepskyblue", "chartreuse3" ) ) + #
theme( plot.title = element_text( size = 14, face = 'bold' ) )
ggplot( sentiment_freq, aes( sentiment, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
ggtitle('Overall Sentiment Distribution') + #
# scale_color_manual( values = c( "indianred1", "deepskyblue", "chartreuse3" ) ) + #
theme( plot.title = element_text( size = 14, face = 'bold' ) )
ggplot( sentiment_freq, aes( sentiment, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
ggtitle('Overall Sentiment Distribution') + #
scale_color_manual( values = c( "black", "deepskyblue", "chartreuse3" ) ) + #
theme( plot.title = element_text( size = 14, face = 'bold' ) )
ggplot( sentiment_freq, aes( sentiment, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
ggtitle('Overall Sentiment Distribution') + #
scale_fill_manual( values = c( "indianred1", "deepskyblue", "chartreuse3" ) ) + #
theme( plot.title = element_text( size = 14, face = 'bold' ) )
library(cowplot)
ggplot( sentiment_freq, aes( sentiment, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
ggtitle('Overall Sentiment Distribution') + #
scale_fill_manual( values = c( "indianred1", "deepskyblue", "chartreuse3" ) ) + #
theme( plot.title = element_text( size = 14, face = 'bold' ) )
sentiment_color <- c( "indianred1", "deepskyblue", "chartreuse3" )#
ggplot( sentiment_freq, aes( sentiment, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
ggtitle('Overall Sentiment Distribution') + #
scale_fill_manual( values = sentiment_color ) + #
theme( plot.title = element_text( size = 14, face = 'bold' ) )
sentiment_per_airline <- with( tweets, table( airline_sentiment, airline ) ) %>%#
                         prop.table() %>%#
                         data.table()#
setnames( sentiment_per_airline, c( 'sentiment', 'airline', 'percentage' ) )#
#
ggplot( sentiment_per_airline, aes( airline, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity", position = 'fill' ) + #
ggtitle('Proportion of Tweets per Airline') +#
theme( plot.title = element_text( size = 14, face = 'bold', hjust = 0.5 ), #
       axis.title.x = element_text( vjust = -1 ) )
ggplot( sentiment_per_airline, aes( airline, percentage, fill = sentiment ) ) + #
geom_bar( stat = "identity", position = 'fill' ) + #
ggtitle('Proportion of Tweets per Airline') +#
scale_fill_manual( values = sentiment_color ) + #
theme( plot.title = element_text( size = 14, face = 'bold', hjust = 0.5 ), #
       axis.title.x = element_text( vjust = -1 ) )
negativereason_per_airline <- with( tweets, table( airline, negativereason ) ) %>%#
                              prop.table() %>%#
                              data.table()#
# some of the negative reasons have been left blank we'll change them to not specified#
negativereason_per_airline[ negativereason == '', negativereason := 'not specified' ]#
#
ggplot( negativereason_per_airline, aes( negativereason, N, fill = airline ) ) + #
geom_bar( stat = "identity" ) + guides( fill = FALSE ) + #
facet_wrap( ~ airline ) + #
theme( plot.title = element_text( size = 14, face = 'bold', vjust = 1 ), #
       axis.text.x = element_text( angle = 90, size = 10, vjust = 1 ) )
table(tweets$retweet_count)#
tweets[ retweet_count == 44, .(text) ]
# location of the tweet#
location <- tweets$tweet_coord#
location <- location[ !is.na(location) ]#
#
# add a count column filled with 1s#
# remove duplicate locations and count the times they appeared#
location <- data.table( count = 1, location = location )#
location <- location[ , .( count = sum(count) ), by = location ][ order(-count) ]#
#
location[ , location := gsub( '\\[(.*)\\]', '\\1', location ) ]#
location[ , c( 'lat', 'long') := tstrsplit( location, ',' ) ]#
location[ , location := NULL ]#
location[ , `:=`( long = as.numeric(long), lat = as.numeric(lat) ) ]#
#
# removes row containing coords [0,0] which are probably wrong#
location <- location[ !( lat == 0 & long == 0 ), ]#
world_map <- map_data("world")#
#
ggplot() + #
geom_polygon( data = world_map, aes( long, lat, group = group ), #
              color = "black", fill = 'lightblue' ) + #
geom_point( data = location, aes( long, lat, size = count ), color = "coral1" ) +#
ggtitle("Location of tweets across the World") + #
ylim( c( -50, 80 ) ) + scale_size( name = "Total Tweets" )

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir('../../notebook_format')\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2016-08-13 14:58:02 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 4.2.0\n",
      "\n",
      "numpy 1.11.1\n",
      "pandas 0.18.1\n",
      "matplotlib 1.5.1\n",
      "keras 1.0.6\n",
      "scikit-learn 0.17.1\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 8, 6 # change default figure size\n",
    "\n",
    "# 1. magic to print version\n",
    "# 2. magic so that the notebook will reload external python modules\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Embedding\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,keras,scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Word Embedding and ConvNets\n",
    "\n",
    "## What are word embeddings?\n",
    "\n",
    "\"Word embeddings\" (or so called word vectors) are a family of natural language processing techniques that aims to map semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space.\n",
    "\n",
    "For instance, \"coconut\" and \"polar bear\" are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But \"kitchen\" and \"dinner\" are related words, so they should be embedded close to each other.\n",
    "\n",
    "Ideally, in a good embeddings space, the \"path\" (a vector) to go from \"kitchen\" and \"dinner\" would capture precisely the semantic relationship between these two concepts. In this case the relationship is \"where x occurs\", so you would expect the vector kitchen - dinner (difference of the two embedding vectors, i.e. path to go from dinner to kitchen) to capture this \"where x occurs\" relationship. Basically, we should have the vectorial identity: dinner + (where x occurs) = kitchen (at least approximately). If that's indeed the case, then we can use such a relationship vector to answer questions. For instance, starting from a new vector, e.g. \"work\", and applying this relationship vector, we should get sometime meaningful, e.g. work + (where x occurs) = office, answering \"where does work occur?\".\n",
    "\n",
    "Word embeddings are computed by applying dimensionality reduction techniques to datasets of co-occurence statistics between words in a corpus of text. This can be done via shallow neural networks, e.g. **word2vec**, or via matrix factorization, e.g. **Glove**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The dataset we'll use is the [20 Newsgroup dataset](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html). The task that we will try to solve is to classify posts coming from 20 different newsgroup into their original 20 categories using text associated with each post.\n",
    "\n",
    "Categories (listed below) are fairly semantically distinct and thus will have quite different words associated with them.\n",
    "\n",
    "```\n",
    "alt.atheism\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\n",
    "talk.politics.misc\n",
    "talk.religion.misc\n",
    "soc.religion.christian\n",
    "\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "\n",
    "rec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\n",
    "\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.space\n",
    "sci.med\n",
    "\n",
    "misc.forsale\n",
    "\n",
    "```\n",
    "\n",
    "To solve the classification problem we will:\n",
    "\n",
    "- Convert all text samples in the dataset into sequences of word indices. A \"word index\" is simply representing each word with an integer ID. We will only consider the top 50,000 most commonly occuring words in the dataset, and we will truncate the sequences to a maximum length of 1000 words (sentences that have less than 1000 words will be padded with 0).\n",
    "- Prepare an \"embedding matrix\" (more detail below) which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "- Load this embedding matrix into a Keras `Embedding` layer, set to be frozen (its weights, the embedding vectors, will not be updated during training).\n",
    "- Build a 1D convolutional neural network on top of it, ending in a softmax output over our 20 categories.\n",
    "\n",
    "> Side note: When solving classic text classication problem (e.g. spam filtering, sentiment analysis) using neural networks, Recurrent neural network (RNN) might be a more 'natural' approach, given that text is naturally sequential. However, RNNs are quite slow and fickle to train and Convnets surprisingly works quite well.\n",
    "\n",
    "**GloVe word embeddings**\n",
    "\n",
    "For the embedding matrix, we will use the pre-trained [GloVe embeddings](http://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a  popular embedding technique based on factorizing a matrix of word co-occurence statistics.\n",
    "\n",
    "Specifically, we will use the pre-trained 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. You can download them [here](http://nlp.stanford.edu/data/glove.6B.zip).\n",
    "\n",
    "<p>\n",
    "<div class=\"alert alert-warning\">\n",
    "The pre-trained word embeddings is a 822MB download\n",
    "</div>\n",
    "\n",
    "With all that being said and done, let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some global variables\n",
    "GLOVE_DIR = 'glove.6B/'\n",
    "TEXT_DATA_DIR = '20_newsgroup/'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Preparing the text data:\n",
    "\n",
    "We will simply iterate over the folders in which our text samples are stored, and format them into a list of data points. We will also assign each data point a class label (It turns out scikit-learn also already has this data built-in so we don't have to do this loading ourselves ...).\n",
    "\n",
    "<p>\n",
    "<div class=\"alert alert-warning\">\n",
    "If you happened to get an UnicodeDecodeError: 'utf8' codec can't decode byte error when reading the dataset, the way to get around this is to add `encoding = 'Latin-1'` to the open file method according to [here](https://github.com/fchollet/keras/pull/3417)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "# 1. list of text for each news story (data point)\n",
    "# 2. list of label ids (target)\n",
    "# 3. dictionary mapping label name (news' category) to numeric id\n",
    "texts  = []\n",
    "labels = []\n",
    "labels_index = {}  \n",
    "\n",
    "for name in os.listdir(TEXT_DATA_DIR):\n",
    "    \n",
    "    # each news category is stored in different category\n",
    "    path = os.path.join( TEXT_DATA_DIR, name )\n",
    "    if os.path.isdir(path):\n",
    "        \n",
    "        # assign a distinct label id for each category\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "    \n",
    "        for fname in os.listdir(path):\n",
    "            fpath = os.path.join( path, fname )\n",
    "            with open( fpath, encoding = 'Latin-1' ) as f:\n",
    "                text = f.read()\n",
    "                texts.append(text)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print( 'Found %s texts.' % len(texts) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we preprocess our text samples and labels using functionalities provided by Kersa so that they can be fed into a neural network. `Tokenizer` allows us to convert the text data into a sequence of integers (building a dictionary mapping word to index then converting the each word in the orginal to index). For more info refer to the [Keras Text Preprocessing Doc](https://keras.io/preprocessing/text/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 214873 unique tokens.\n",
      "CPU times: user 11.2 s, sys: 102 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# keras preprocessing\n",
    "tokenizer = Tokenizer(nb_words = 50000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# a dictionary of words to their id (all the words)\n",
    "word_index = tokenizer.word_index\n",
    "print( 'Found %s unique tokens.' % len(word_index) )\n",
    "\n",
    "# during the text_to_sequence, it will remove words\n",
    "# whose frequency is not larger than the `nb_words` parameter\n",
    "# that you've specified\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 unique tokens.\n",
      "CPU times: user 9.82 s, sys: 63.6 ms, total: 9.89 s\n",
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# write a different preprocessing\n",
    "from text import Tokenizer as Tokenizer2\n",
    "tokenizer2 = Tokenizer2(nb_words = 50000)\n",
    "tokenizer2.fit_on_texts(texts)\n",
    "\n",
    "# a dictionary of words to their id (all the words)\n",
    "word_index = tokenizer2.word_index\n",
    "print( 'Found %s unique tokens.' % len(word_index) )\n",
    "\n",
    "# during the text_to_sequence, it will remove words\n",
    "# whose frequency is not larger than the `nb_words` parameter\n",
    "# that you've specified\n",
    "sequences = tokenizer2.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (19997, 1000)\n",
      "Shape of label: (19997, 20)\n",
      "Shape of training data: (15998, 1000)\n",
      "Shape of validation data: (3999, 1000)\n",
      "Shape of training label: (15998, 20)\n",
      "Shape of validation label: (3999, 20)\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences so they become a equal lengthed array\n",
    "# this will be our preprocessed data\n",
    "X = pad_sequences( sequences, maxlen = MAX_SEQUENCE_LENGTH )\n",
    "\n",
    "# one-hot encode the y labels\n",
    "y = to_categorical(np.array(labels))\n",
    "print('Shape of data:', X.shape)\n",
    "print('Shape of label:', y.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "np.random.seed(1337)\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "val_samples = int(VALIDATION_SPLIT * X.shape[0])\n",
    "\n",
    "X_train = X[:-val_samples]\n",
    "y_train = y[:-val_samples]\n",
    "X_val = X[-val_samples:]\n",
    "y_val = y[-val_samples:]\n",
    "print('Shape of training data:', X_train.shape)\n",
    "print('Shape of validation data:', X_val.shape)\n",
    "print('Shape of training label:', y_train.shape)\n",
    "print('Shape of validation label:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Embedding layer\n",
    "\n",
    "We'll read in one line of the glove dataset to get a feeling of what it looks like. Note that the file name is `100d`, there're different dimensions of word embeddings in the file. you can try which one leads to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOVE_FILE = os.path.join( GLOVE_DIR, 'glove.6B.100d.txt' )\n",
    "with open(GLOVE_FILE) as f:\n",
    "    test = next(f)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk reads in the pre-trained embeddings and construct an `embedding_index`, which is a dictionary mapping words to known embeddings. \n",
    "\n",
    "We can then leverage our `embedding_index` dictionary and our `word_index` to compute our embedding matrix. By computing, we mean if the word appeared in the pre-trained word embedding, then it will use its coefficients, if not then the coefficients' value will be set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "# words stored as keys, \n",
    "# corresponding embeddings (vectors) stored as values\n",
    "embeddings_index = {}\n",
    "GLOVE_FILE = os.path.join( GLOVE_DIR, 'glove.6B.100d.txt' )\n",
    "with open(GLOVE_FILE) as f:\n",
    "    for line in f:\n",
    "        value = line.split()\n",
    "        word  = value[0]\n",
    "        coefs = np.array( value[1:], dtype = 'float32' )\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print( 'Found %s word vectors.' % len(embeddings_index) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34313\n"
     ]
    }
   ],
   "source": [
    "# 1. the EMBEDDING_DIM is 100, which matches the 100d glove file name\n",
    "# 2. add one to the np.array, since the word id starts from 1 instead of 0\n",
    "count = 0\n",
    "embedding_matrix = np.zeros( ( len(word_index) + 1, EMBEDDING_DIM ), dtype = 'float32' )\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count += 1\n",
    "\n",
    "# number of tokens that appeared in the pre-trained word vectors\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a 1D convnet\n",
    "\n",
    "Finally we can build a toy 1D convnet to solve our classification problem. Note that for the `Embedding` layer, we set `trainable = False` to prevent the weights from being updated during training.\n",
    "\n",
    "An `Embedding` layer should be fed sequences of integers, i.e. a 2D input of shape (samples, indices). These input sequences should be padded so that they all have the same length in a batch of input data (although an `Embedding` layer is capable of processing sequence of heterogenous length, if you don't pass an explicit `input_length` argument to the layer).\n",
    "\n",
    "All the `Embedding` layer does is to map the integer inputs to the vectors found at the corresponding index in the embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]. This means that the output of the Embedding layer will be a 3D tensor of shape (samples, sequence_length, embedding_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = y_train.shape[1]\n",
    "\n",
    "# setting up the Embedding layer\n",
    "embedding_layer = Embedding( len(word_index) + 1, EMBEDDING_DIM,\n",
    "                             weights = [embedding_matrix],\n",
    "                             input_length = MAX_SEQUENCE_LENGTH,\n",
    "                             trainable = False )\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D( nb_filter = 128, filter_length = 5, activation = 'relu' ))\n",
    "model.add(MaxPooling1D( pool_length = 5 ))\n",
    "model.add(Conv1D( nb_filter = 128, filter_length = 5, activation = 'relu' ))\n",
    "model.add(MaxPooling1D( pool_length = 5 ))\n",
    "model.add(Conv1D( nb_filter = 128, filter_length = 5, activation = 'relu' ))\n",
    "model.add(MaxPooling1D( pool_length = 35 ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense( 128, activation = 'relu' ))\n",
    "model.add(Dense( n_classes, activation = 'softmax' ))\n",
    "model.compile( loss = 'categorical_crossentropy',\n",
    "               optimizer = 'adam', metrics = ['acc'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# obtain the graph visualization of the network if we wish\n",
    "from keras.utils.visualize_util import plot\n",
    "plot( model, to_file = 'model.png', show_shapes = True, show_layer_names = True )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/6\n",
      "15998/15998 [==============================] - 415s - loss: 2.3735 - acc: 0.2363 - val_loss: 1.4499 - val_acc: 0.4626\n",
      "Epoch 2/6\n",
      "15998/15998 [==============================] - 435s - loss: 0.8215 - acc: 0.7264 - val_loss: 0.3858 - val_acc: 0.8755\n",
      "Epoch 3/6\n",
      "15998/15998 [==============================] - 434s - loss: 0.2391 - acc: 0.9284 - val_loss: 0.1963 - val_acc: 0.9397\n",
      "Epoch 4/6\n",
      "15998/15998 [==============================] - 427s - loss: 0.1413 - acc: 0.9509 - val_loss: 0.1675 - val_acc: 0.9435\n",
      "Epoch 5/6\n",
      "15998/15998 [==============================] - 435s - loss: 0.1177 - acc: 0.9550 - val_loss: 0.1496 - val_acc: 0.9435\n",
      "Epoch 6/6\n",
      "15998/15998 [==============================] - 437s - loss: 0.1045 - acc: 0.9587 - val_loss: 0.1318 - val_acc: 0.9490\n",
      "CPU times: user 4h 30min 31s, sys: 18min 13s, total: 4h 48min 44s\n",
      "Wall time: 43min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# if the validation set's loss does not improvement\n",
    "# after 2 epochs (patience), the training will be stopped\n",
    "callback = [ EarlyStopping( monitor = 'val_loss', patience = 2, verbose = 1 ) ]\n",
    "model.fit( X_train, y_train, shuffle = True,\n",
    "           nb_epoch = 6, batch_size = 256,\n",
    "           validation_data = (X_val, y_val),\n",
    "           callbacks = callback )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999/3999 [==============================] - 46s    \n"
     ]
    }
   ],
   "source": [
    "# predict the validation set's accuracy\n",
    "y_val_pred = model.predict_classes( X_val, verbose = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 94.90\n"
     ]
    }
   ],
   "source": [
    "# accuracy score for the word embedding / covnets\n",
    "y_labels = np.array(labels)[indices][-val_samples:]\n",
    "accuracy = np.sum( y_labels == y_val_pred ) / X_val.shape[0]\n",
    "print( 'valid accuracy: %.2f' % ( accuracy * 100 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model reaches 94% classification accuracy on the validation set after 6 epochs. \n",
    "\n",
    "There're a whole bunch of other different things that we can try out to get an even higher accuracy:\n",
    "\n",
    "- Include regularization mechanism (e.g. dropout)\n",
    "- Try tuning the embedding layer (e.g. load different dimensions of pre-trained word vectors or let it train with the network using more epochs)\n",
    "- Different network architecture, including increasing the number of epochs as the early stopping criteria was not met\n",
    "\n",
    "To test how well we would have performed by not using pre-trained word embeddings, we just need to replace our Embedding layer with the following:\n",
    "\n",
    "```python\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length = MAX_SEQUENCE_LENGTH )\n",
    "```\n",
    "\n",
    "This will initialize our `Embedding` layer from scratch and let it learn its weights during training.\n",
    "\n",
    "In general, using pre-trained embeddings is relevant for natural processing tasks were little training data is available. The functionally the embeddings act as an injection of outside information, which might prove useful for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Performance\n",
    "\n",
    "One popular approach to tackle this type of text classification problem is to use bag-of-words of tf-idf for the feature engineering step and and train a logistic (softmax) regression classifier. This combination serves as a very strong baseline and getting other approaches to match its speed/accuracy is difficult. We'll try one here, and compare the performances to see if the doing extra work like word embedding and training a more complicated was worth the effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# used the shuffled indices from before\n",
    "# to shuffle the original text and label\n",
    "texts = np.array(texts)\n",
    "labels = np.array(labels)\n",
    "texts_shuffled  = texts[indices]\n",
    "labels_shuffled = labels[indices]\n",
    "\n",
    "# train / validation split\n",
    "texts_train  = texts_shuffled[:-val_samples]\n",
    "labels_train = labels_shuffled[:-val_samples]\n",
    "texts_val  = texts_shuffled[-val_samples:]\n",
    "labels_val = labels_shuffled[-val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15998, 476088)\n"
     ]
    }
   ],
   "source": [
    "# remove built-in english stop words, use 1 and 2 gram and remove words\n",
    "# that appeared in less than two documents\n",
    "tfidf = TfidfVectorizer( stop_words = 'english', ngram_range = (1, 2), min_df = 2 )\n",
    "X_train_tfidf = tfidf.fit_transform(texts_train)\n",
    "X_val_tfidf = tfidf.transform(texts_val)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 2.37 s, total: 1min 33s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(n_jobs = -1)\n",
    "logreg.fit( X_train_tfidf, labels_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951987996999\n"
     ]
    }
   ],
   "source": [
    "# accuracy score for the tfidf-logistic regression\n",
    "y_val_pred = logreg.predict(X_val_tfidf)\n",
    "print( accuracy_score( labels_val, y_val_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Keras Blog: Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "- [Source code of the blog above](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "- [Keras Text Preprocessing Doc](https://keras.io/preprocessing/text/)\n",
    "- [Keras Text Preprocessing Source Code](https://github.com/fchollet/keras/tree/master/keras/preprocessing)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

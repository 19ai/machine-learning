{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "\n",
       "    div.cell{\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "   }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_subarea.output_text.output_pyout {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_subarea.output_stream.output_stdout.output_text {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "}\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir('../notebook_format')\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "**Naive Bayes** classifiers is based on Bayesâ€™ theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but **Naive Bayes** still tend to perform very well in the fields of text/document classification. Common applications includes spam filtering (categorized a text message as spam or not-spam) and sentiment analysis (categorized a text message as positive or negative review). More importantly, the simplicity of the method means that it takes order of magnitude less time to train when compared to more complext models like support vector machines.\n",
    "\n",
    "\n",
    "## Text/Document Representations\n",
    "\n",
    "Text classifiers often donâ€™t use any kind of deep representation about language: often times a document is represented as a bag of words. (A bag is like a set that allows repeating elements.) This is an extremely simple representation as it throws away the word order and only keeps which words are included in the document and how many times each word occurs.\n",
    " \n",
    "We shall look at two probabilistic models of documents, both of which represent documents as a bag of words, using the **Naive Bayes** assumption. Both models represent documents using feature vectors\n",
    "whose components correspond to word types. If we have a document containing $|V|$ distinct vocabularies,\n",
    "then the feature vector dimension $d=|V|$.\n",
    "\n",
    "- **Bernoulli document model:** a document is represented by a feature vector with binary elements taking value 1 if the corresponding word is present in the document and 0 if the word is not present.\n",
    "- **Multinomial document model:** a document is represented by a feature vector with integer elements whose value is the frequency of that word in the document.\n",
    "\n",
    "Example: Consider the vocabulary V = {blue,red, dog, cat, biscuit, apple}. In this case |V| = d = 6. Now consider the (short) document \"the blue dog ate a blue biscuit\". If $d^B$ is the **Bernoulli** feature vector for this document, and $d^M$ is the **Multinomial** feature vector, then we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoulli [1, 0, 1, 0, 1, 0]\n",
      "multinomial [2, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab = [ 'blue', 'red', 'dog', 'cat', 'biscuit', 'apple' ]\n",
    "doc = \"the blue dog ate a blue biscuit\"\n",
    "\n",
    "# note that the words that didn't appear in the vocabulary will be discarded\n",
    "bernoulli = [ 1 if v in doc else 0 for v in vocab ]\n",
    "multinomial = [ doc.count(v) for v in vocab ]\n",
    "print( 'bernoulli', bernoulli )\n",
    "print( 'multinomial', multinomial )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Model\n",
    "\n",
    "Consider a document $D$, whose class is given by $C = 1, 2, ..., K$. We classify D as the class which has the highest posterior probability $argmax_{ k = 1, 2, ..., K} \\, P(C = k|D)$, which can be re-expressed using Bayesâ€™ Theorem:\n",
    "\n",
    "$$p(C = k|D) = \\frac{ p(C = k) \\, p(D|C = k) }{p(D)} \\ \\propto p(C = k) \\, p(D|C = k)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\propto$ means is proportional to.\n",
    "- $p(C = k)$ represents the class k's prior probabilities.\n",
    "- $p(D|C = k)$ is the likelihoods of the document given the class k.\n",
    "- $p(D)$ is the noramlizing factor which we don't have to compute since it does not depend on the class $C$, or meaning that the it's same for all class $C$.\n",
    "\n",
    "Starting with $p(D|C)$. The spirit of **Naive Bayes** is it assumes that each of the features it uses are conditionally independent of one another given some class. More formally, if we wish to calculate the probability of observing features $X_1$ through $X_d$, given some class $C$ we can do it by the following math formula:\n",
    "\n",
    "$$p(x_{1},x_{2},...,x_{d} \\mid C) = \\prod_{i=1}^{d}p(x_{i} \\mid C)$$ \n",
    "\n",
    "Now if we have a vocabulary (features) $V$ containing a set of $|V|$ words and the $t_{th}$ dimension of a document vector corresponds to word $w_t$ in the vocabulary. If we follow the **Naive Bayes** assumption, that the probability of each word occurring in the document is independent of the occurrences of the other words, then we can re-write the $i_{th}$ document likelihood $p(D_i \\mid C)$ as:\n",
    "\n",
    "$$p(D_i \\mid C ) = \\prod_{t=1}^{d}b_{it}p(w_t \\mid C) + ( 1 - b_{it} ) (1- p(w_t \\mid C)) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $p(w_t \\mid C)$ is the probability of word $w_t$ occurring in a document of class $C$.\n",
    "- $1- p(w_t \\mid C)$ is the probability of $w_t$ not occurring in a document of class $C$.\n",
    "- $b_{it}$ is either 0 or 1 representing the absence or presence of word $w_t$ in the $i_{th}$ document.\n",
    "\n",
    "This product goes over all words in the vocabulary. If word $w_t$ is present, then $b_{it} = 1$ and the associated probability is $p(w_t \\mid C)$; If word $w_t$ is not present, then $b_{it} = 0$ and the associated probability becomes $1- p(w_t \\mid C)$.\n",
    "\n",
    "\n",
    "As for the word likelihood $p(w_t \\mid C)$, we can learn (estimate) these parameters from a training set of documents labelled with class $C=k$.\n",
    "\n",
    "$$p(w_t \\mid C = k) = \\frac{n_k(w_t)}{N_k}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $n_k(w_t)$ is the number of class $C=k$'s document in which $w_t$ is observed.\n",
    "- $N_k$ is the number of documents that belongs to class $k$.\n",
    "\n",
    "Last, calculating $p(C)$ is relatively simple: If there are $N$ documents in total in the training set, then the prior probability of class $C=k$ may be estimated as the relative frequency of documents of class $C=k$:\n",
    "\n",
    "$$p(C = k)\\,= \\frac{N_k}{N}$$\n",
    "\n",
    "Where $N$ is the total number of documents in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Model Example\n",
    "\n",
    "Consider a set of documents, each of which is related either to Class 1 or to Class 0. Given\n",
    "a training set of 11 documents, we would like to train a Naive Bayes classifier, using the Bernoulli\n",
    "document model, to classify unlabelled documents as Class 1 or 0. We define a vocabulary of eight words.\n",
    "\n",
    "Thus the training data $X$ is presented below as a 11*8 matrix, in which each row represents an 8-dimensional document vector. And the $y$ represents the class of each document. Then we would like to classify the two testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:\n",
      "[[1 0 0 0 1 1 1 1]\n",
      " [0 0 1 0 1 1 0 0]\n",
      " [0 1 0 1 0 1 1 0]\n",
      " [1 0 0 1 0 1 0 1]\n",
      " [1 0 0 0 1 0 1 1]\n",
      " [0 0 1 1 0 0 1 1]\n",
      " [0 1 1 0 0 0 1 0]\n",
      " [1 1 0 1 0 0 1 1]\n",
      " [0 1 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 1 0]]\n",
      "\n",
      "[1 1 1 1 1 1 0 0 0 0 0]\n",
      "\n",
      "testing data:\n",
      "[[1 0 0 1 1 1 0 1]\n",
      " [0 1 1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "train = np.genfromtxt( 'train.txt', dtype = np.int )\n",
    "X_train = train[ :, :-1 ]\n",
    "y_train = train[ :, -1 ] # the last column is the class\n",
    "print('training data:')\n",
    "print(X_train)\n",
    "print()\n",
    "print(y_train)\n",
    "print()\n",
    "print('testing data:')\n",
    "X_test = np.array([ [1, 0, 0, 1, 1, 1, 0, 1], [0, 1, 1, 0, 1, 0, 1, 0] ])\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bernoulli_nb( X_train, y_train, X_test ):\n",
    "    \"\"\"\n",
    "    Pass in the training data, it's label and \n",
    "    predict the testing data's class\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the prior proabability p(C=k)\n",
    "    N = X_train.shape[0]\n",
    "    priors = np.bincount(y_train) / N\n",
    "    \n",
    "    # obtain the unique class's type (since it may not be 0 and 1)\n",
    "    class_type = np.unique(y_train)\n",
    "    class_nums = class_type.shape[0]\n",
    "    word_likelihood = np.zeros( ( class_nums, X_train.shape[1] ) )\n",
    "    \n",
    "    # calculate the word likelihood p(w_tâˆ£C)\n",
    "    for index, output in enumerate(class_type):\n",
    "        subset = X_train[ np.equal( y_train, output ) ]\n",
    "        word_likelihood[ index, : ] = np.sum( subset, axis = 0 ) / subset.shape[0]\n",
    "        \n",
    "    # make predictions on the test set\n",
    "    # note that this code will break if there's only one test set\n",
    "    # since the first for loop will not be looping through each document\n",
    "    # but each document's feature\n",
    "    predictions = np.zeros( X_test.shape[0], dtype = np.int )\n",
    "    for index1, document in enumerate(X_test):\n",
    "\n",
    "        posteriors = np.zeros(class_nums)\n",
    "        \n",
    "        # calculate p(C = k|D) for the document for all class\n",
    "        # and return the predicted class with the maximum probability\n",
    "        for c in range(class_nums):\n",
    "            \n",
    "            # start with p(C = k)\n",
    "            posterior = priors[c]\n",
    "            word_likelihood_subset = word_likelihood[ c, : ]\n",
    "            \n",
    "             # loop through features to calculate p(Dâˆ£C = k)\n",
    "            for index2, feature in enumerate(document):\n",
    "\n",
    "                if feature:\n",
    "                    prob = word_likelihood_subset[index2]\n",
    "                else:\n",
    "                    prob = 1 - word_likelihood_subset[index2]\n",
    "                posterior *= prob\n",
    "\n",
    "            posteriors[c] = posterior\n",
    "\n",
    "        predicted_class = class_type[ np.argmax(posteriors) ]\n",
    "        predictions[index1] = predicted_class\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = bernoulli_nb( X_train, y_train, X_test )\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Distribution\n",
    "\n",
    "Before discussing the multinomial document model, it is important to be familiar with the multinomial\n",
    "distribution. We first need to be able to count the number of distinct arrangements of a set of items, when some of the items are indistinguishable. For example: Using all the letters, how many distinct sequences can you make from the word \"Mississippi\"? There are 11 letters to permute, but \"i\" and \"s\" occur four times, \"p\" twice and \"M\" once. This gives the total number distinct permutations as:\n",
    "\n",
    "$$\\frac{11!}{4!4!2!1!}$$\n",
    "\n",
    "Generally if we have $n$ items of $d$ types, such that $n_1 + n_2 + ... + n_d = n$, then the number of distinct permutations is given by:\n",
    "\n",
    "$$\\frac{n!}{n_1!n_2!...n_d!}$$\n",
    "\n",
    "Suppose a population contains items of $d \\geq 2$ different types and that the proportion of items that\n",
    "are of type $t$ is $p_t, t=1, ..., d$, with\n",
    "\n",
    "$$\\sum_{t=1}^d p_t =1 \\\\\\, p_t > 0, \\text{for all } t$$\n",
    "\n",
    "Then the probability of \n",
    "\n",
    "If all of that is still unclear, try this [Youtube Video on Introduction to the Multinomial Distribution](https://www.youtube.com/watch?v=syVW7DgvUaY)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Model\n",
    "\n",
    "In the multinomial document model, the document feature vectors capture the frequency of words, not\n",
    "just their presence or absence. Let $x_i$ be the multinomial model feature vector for the $i_{th}$ document $D_i$. The $t_{th}$ element of $x_i$, written $x_{it}$, is the count of the number of times word $w_t$ occurs in document $D_i$. Thus $n_i= \\sum_t x_{it}$ is the total number of words in document $D_i$.\n",
    "\n",
    "Let $p(w_t \\mid C)$ again be the probability of word $w_t$ occurring in a document of class $C$. This time estimated using the word\n",
    "frequency information from the document feature vectors. We again make the naive Bayes assumption,\n",
    "that the probability of each word occurring in the document is independent of the occurrences of\n",
    "the other words. We can then write the document likelihood P(D_i|C) as a multinomial distribution, where the number of draws corresponds to the length of the document, and the proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  label_num\n",
      "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
      "1   ham                      Ok lar... Joking wif u oni...          0\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
      "3   ham  U dun say so early hor... U c already then say...          0\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...          0\n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...          1\n"
     ]
    }
   ],
   "source": [
    "path = 'data/sms.tsv'\n",
    "sms = pd.read_table( path, header = None, names = ['label', 'message'] )\n",
    "sms['label_num'] = sms['label'].map({ 'ham': 0, 'spam': 1 })\n",
    "\n",
    "# extract the first 6 rows as example\n",
    "X = sms.loc[ :6, 'message' ]\n",
    "y = sms.loc[ :6, 'label_num' ]\n",
    "sms.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'like', 'ok', 'std', 'there', 'to']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 3],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 2],\n",
       "       [0, 2, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retain only words that appear that least in two documents\n",
    "vect = CountVectorizer( min_df = 2 )\n",
    "X_dtm = vect.fit_transform(X).toarray()\n",
    "print( vect.get_feature_names() )\n",
    "X_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Notes on Text Classification using Naive Bayes](http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf)\n",
    "- [Youtube Video on Introduction to the Multinomial Distribution](https://www.youtube.com/watch?v=syVW7DgvUaY)\n",
    "\n",
    "http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Paper: Improved Naive Bayes on Sentiment Analysis](https://github.com/vivekn/sentiment)\n",
    "\n",
    "\n",
    "[Sebastian Raschka's blog on Naive Bayes and Text Classification](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "https://github.com/wepe/MachineLearning/tree/master/NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

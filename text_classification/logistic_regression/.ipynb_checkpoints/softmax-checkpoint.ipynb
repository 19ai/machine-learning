{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "\n",
       "    div.cell{\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "   }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_subarea.output_text.output_pyout {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_subarea.output_stream.output_stdout.output_text {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "}\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir('../../notebook_format')\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "**Softmax Regression** (synonyms: *Multinomial Logistic*, *Maximum Entropy Classifier*, or just *Multi-class Logistic Regression*) is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are  mutually exclusive). In contrast, we use the (standard) *Logistic Regression* model in binary classification tasks.\n",
    "\n",
    "In **Softmax Regression**, we replace the sigmoid logistic function by the so-called *softmax* function $\\phi_{softmax}(\\cdot)$.\n",
    "\n",
    "$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=1}^{k} e^{z_{j}^{(i)}}}$$\n",
    "\n",
    "where we define the net input *z* as \n",
    "\n",
    "$$z = w_1x_1 + ... + w_mx_m  + b= \\sum_{l=1}^{m} w_l x_l + b= \\mathbf{w}^T\\mathbf{x} + b$$ \n",
    "\n",
    "(**w** is the weight vector, $\\mathbf{x}$ is the feature vector of 1 training sample. Each $w$ corresponds to a feature $x$ and there're $m$ of them in total. $b$ is the bias unit. $k$ denotes the total number of classes.)   \n",
    "\n",
    "Now, this softmax function computes the probability that the $i_{th}$ training sample $\\mathbf{x}^{(i)}$ belongs to class $l$ given the weight and net input $z^{(i)}$. So, we compute the probability $p(y = j \\mid \\mathbf{x^{(i)}; w}_j) $ for each class label in $j = 1, \\ldots, k$. Note the normalization term in the denominator which causes these class probabilities to sum up to one.\n",
    "\n",
    "![](softmax.png)\n",
    "\n",
    "To illustrate the concept of softmax, let us walk through a concrete example. Suppose we have a training set consisting of 4 samples from 3 different classes (0, 1, and 2)\n",
    "\n",
    "- $x_0 \\rightarrow \\text{class }0$\n",
    "- $x_1 \\rightarrow \\text{class }1$\n",
    "- $x_2 \\rightarrow \\text{class }2$\n",
    "- $x_3 \\rightarrow \\text{class }2$\n",
    "\n",
    "First, we apply one-hot encoding to encode the class labels into a format that we can more easily work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([ 0, 1, 2, 2 ])\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    class_num = np.unique(y).shape[0]\n",
    "    y_encode = np.zeros( ( y.shape[0], class_num ) )\n",
    "    for idx, val in enumerate(y):\n",
    "        y_encode[ idx, val ] = 1.0\n",
    "    \n",
    "    return y_encode\n",
    "\n",
    "y_encode = one_hot_encode(y)\n",
    "y_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample that belongs to class 0 (the first row) has a 1 in the first cell, a sample that belongs to class 1 has a 1 in the second cell of its row, and so forth.\n",
    "\n",
    "Next, let us define the feature matrix of our 4 training samples. Here, we assume that our dataset consists of 2 features; thus, we create a 4x2 dimensional matrix of our samples and features.\n",
    "Similarly, we create a 2x3 dimensional weight matrix (one row per feature and one column for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs X:\n",
      " [[ 0.1  0.5]\n",
      " [ 1.1  2.3]\n",
      " [-1.1 -2.3]\n",
      " [-1.5 -2.5]]\n",
      "\n",
      "Weights W:\n",
      " [[ 0.1  0.2  0.3]\n",
      " [ 0.1  0.2  0.3]]\n",
      "\n",
      "bias:\n",
      " [ 0.01  0.1   0.1 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.1, 0.5],\n",
    "              [1.1, 2.3],\n",
    "              [-1.1, -2.3],\n",
    "              [-1.5, -2.5]])\n",
    "\n",
    "W = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "\n",
    "bias = np.array([0.01, 0.1, 0.1])\n",
    "\n",
    "print('Inputs X:\\n', X)\n",
    "print('\\nWeights W:\\n', W)\n",
    "print('\\nbias:\\n', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the net input, we multiply the 4x2 matrix feature matrix `X` with the 2x3 (n_features x n_classes) weight matrix `W`, which yields a 4x3 output matrix (n_samples x n_classes) to which we then add the bias unit: \n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net input:\n",
      " [[ 0.07  0.22  0.28]\n",
      " [ 0.35  0.78  1.12]\n",
      " [-0.33 -0.58 -0.92]\n",
      " [-0.39 -0.7  -1.1 ]]\n"
     ]
    }
   ],
   "source": [
    "def net_input( X, W, b ):\n",
    "    return X.dot(W) + b\n",
    "\n",
    "net_in = net_input( X, W, bias )\n",
    "print('net input:\\n', net_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to compute the softmax activation that we discussed earlier:\n",
    "\n",
    "$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=1}^{k} e^{z_{j}^{(i)}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:\n",
      " [[ 0.29450637  0.34216758  0.36332605]\n",
      " [ 0.21290077  0.32728332  0.45981591]\n",
      " [ 0.42860913  0.33380113  0.23758974]\n",
      " [ 0.44941979  0.32962558  0.22095463]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum( np.exp(z), axis = 1, keepdims = True )\n",
    "\n",
    "smax = softmax(net_in)\n",
    "print('softmax:\\n', smax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values for each sample (row) nicely sum up to 1 now. E.g., we can say that the first sample  `[ 0.29450637  0.34216758  0.36332605]` has a 29.45% probability to belong to class 0. Now, in order to turn these probabilities back into class labels, we could simply take the argmax-index position of each row:\n",
    "\n",
    "[[ 0.29450637  0.34216758  **0.36332605**] -> 2   \n",
    "[ 0.21290077  0.32728332  **0.45981591**]  -> 2  \n",
    "[ **0.42860913**  0.33380113  0.23758974]  -> 0  \n",
    "[ **0.44941979**  0.32962558  0.22095463]] -> 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  [2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax( axis = 1 )\n",
    "\n",
    "print( 'predicted class labels: ', to_classlabel(smax) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our predictions are terribly wrong, since the correct class labels are `[0, 1, 2, 2]`. Now, in order to train our logistic model (e.g., via an optimization algorithm such as gradient descent), we need to define a cost function $J(\\cdot)$ that we want to minimize:\n",
    "\n",
    "$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H( T^{(i)}, O^{(i)} )$$\n",
    "\n",
    "which is the average of all cross-entropies over our $n$ training samples. The cross-entropy  function is defined as\n",
    "\n",
    "$$H( T^{(i)}, O^{(i)} ) = -\\sum_m T^{(i)} \\cdot log(O^{(i)})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $T$ stands for \"target\" (i.e., the *true* class labels) \n",
    "- $O$ stands for output -- the computed *probability* via softmax; **not** the predicted class label.\n",
    "- $\\sum_m$ denotes adding up the difference between the target and the output for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Cost: 1.32159787159\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_cost( output, y_target ):\n",
    "    return np.mean( -np.sum( np.log(output) * y_target, axis = 1 ) )\n",
    "\n",
    "cost = cross_entropy_cost( output = smax, y_target = y_encode )\n",
    "print('Cross Entropy Cost:', cost )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn the weight for our softmax model via gradient descent, we then need to compute the gradient of our cost function for each class $j \\in \\{0, 1, ..., k\\}$.\n",
    "\n",
    "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$\n",
    "\n",
    "We won't be going through the tedious details here, but this cost's gradient turns out to be simply:\n",
    "\n",
    "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum^{n}_{i=0} \\big[\\mathbf{x}^{(i)}_j\\ \\big( O^{(i)} - T^{(i)} \\big) \\big]$$\n",
    "\n",
    "We can then use the cost derivate to update the weights in opposite direction of the cost gradient with learning rate $\\eta$:\n",
    "\n",
    "$$\\mathbf{w}_j := \\mathbf{w}_j - \\eta \\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$ \n",
    "\n",
    "(note that $\\mathbf{w}_j$ is the weight vector for the class $y=j$), and we update the bias units using:\n",
    "\n",
    "$$\n",
    "\\mathbf{b}_j := \\mathbf{b}_j   - \\eta \\bigg[ \\frac{1}{n} \\sum^{n}_{i=0} \\big( O^{(i)} - T^{(i)} \\big) \\bigg]\n",
    "$$ \n",
    " \n",
    "\n",
    "As a penalty against complexity, an approach to reduce the variance of our model and decrease the degree of overfitting by adding additional bias, we can further add a regularization term such as the L2 term with the regularization parameter $\\lambda$:\n",
    "    \n",
    "$$\\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$$\n",
    "\n",
    "where $||\\mathbf{w}||_{2}^{2}$ simply means adding up the squared weights across all the features and classes.\n",
    "\n",
    "$$||\\mathbf{w}||_{2}^{2} = \\sum^{m}_{l=0} \\sum^{k}_{j=0} w_{l, j}^2$$\n",
    "\n",
    "so that our cost function becomes\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H( T^{(i)}, O^{(i)} ) + \\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}\n",
    "$$\n",
    "\n",
    "and we define the \"regularized\" weight update as\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_j := \\mathbf{w}_j -  \\eta \\big[\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}) + \\lambda \\mathbf{w}_j \\big]\n",
    "$$\n",
    "\n",
    "Note that we don't regularize the bias term, thus the update function for it stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression Code\n",
    "\n",
    "Bringing the concepts together, we could come up with an implementation as follows: Note that for the weight and bias parameter, we'll have initialize a value for it. Here we'll simply draw the weights from a normal distribution and set the bias as zero. The code can be obtained [here](https://github.com/ethen8181/machine-learning/blob/master/text_classification/logistic_regression/softmax.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "feature_nums = 2\n",
    "X = iris.data[:, :feature_nums ] # use only two feature as example\n",
    "y = iris.target\n",
    "\n",
    "# standardize\n",
    "for i in range(feature_nums):\n",
    "    X[ :, i ] = ( X[ :, i ] - X[ :, i ].mean() ) / X[ :, i ].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<softmax.SoftmaxRegression at 0x11b087780>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from softmax import SoftmaxRegression\n",
    "softmax_reg = SoftmaxRegression( eta = 0.01, epochs = 30, minibatches = y.shape[0] )\n",
    "softmax_reg.fit( X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAESCAYAAAAIfCk9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b087c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( range( len(softmax_reg.costs) ), softmax_reg.costs )\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81333333333333335"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = softmax_reg.predict(X)\n",
    "accuracy = np.sum( y_pred == y ) / y.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79333333333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit( X, y )\n",
    "y_pred = log_reg.predict(X)\n",
    "accuracy_score( y_true = y, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Sebastian Raschka - Softmax Regression](http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/bonus/softmax-regression.ipynb)\n",
    "- [TensorFlow - Softmax Regression](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "\n",
       "    div.cell{\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "   }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_subarea.output_text.output_pyout {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_subarea.output_stream.output_stdout.output_text {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "}\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir('../../notebook_format')\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "**Softmax Regression** (synonyms: *Multinomial Logistic*, *Maximum Entropy Classifier*, or just *Multi-class Logistic Regression*) is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are  mutually exclusive). In contrast, we use the (standard) *Logistic Regression* model in binary classification tasks.\n",
    "\n",
    "In **Softmax Regression**, we replace the sigmoid logistic function by the so-called *softmax* function $\\phi_{softmax}(\\cdot)$.\n",
    "\n",
    "$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=1}^{k} e^{z_{j}^{(i)}}}$$\n",
    "\n",
    "where we define the net input *z* as \n",
    "\n",
    "$$z = w_1x_1 + ... + w_mx_m  + b= \\sum_{l=1}^{m} w_l x_l + b= \\mathbf{w}^T\\mathbf{x} + b$$ \n",
    "\n",
    "(**w** is the weight vector, $\\mathbf{x}$ is the feature vector of 1 training sample. Each $w$ corresponds to a feature $x$ and there're $m$ of them in total. $b$ is the bias unit. $k$ denotes the total number of classes.)   \n",
    "\n",
    "Now, this softmax function computes the probability that the $i_{th}$ training sample $\\mathbf{x}^{(i)}$ belongs to class $l$ given the weight and net input $z^{(i)}$. So, we compute the probability $p(y = j \\mid \\mathbf{x^{(i)}; w}_j) $ for each class label in $j = 1, \\ldots, k$. Note the normalization term in the denominator which causes these class probabilities to sum up to one.\n",
    "\n",
    "![](softmax.png)\n",
    "\n",
    "To illustrate the concept of softmax, let us walk through a concrete example. Suppose we have a training set consisting of 4 samples from 3 different classes (0, 1, and 2)\n",
    "\n",
    "- $x_0 \\rightarrow \\text{class }0$\n",
    "- $x_1 \\rightarrow \\text{class }1$\n",
    "- $x_2 \\rightarrow \\text{class }2$\n",
    "- $x_3 \\rightarrow \\text{class }2$\n",
    "\n",
    "First, we apply one-hot encoding to encode the class labels into a format that we can more easily work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([ 0, 1, 2, 2 ])\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    class_num = np.unique(y).shape[0]\n",
    "    y_encode = np.zeros( ( y.shape[0], class_num ) )\n",
    "    for idx, val in enumerate(y):\n",
    "        y_encode[ idx, val ] = 1.0\n",
    "    \n",
    "    return y_encode\n",
    "\n",
    "y_encode = one_hot_encode(y)\n",
    "y_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample that belongs to class 0 (the first row) has a 1 in the first cell, a sample that belongs to class 1 has a 1 in the second cell of its row, and so forth.\n",
    "\n",
    "Next, let us define the feature matrix of our 4 training samples. Here, we assume that our dataset consists of 2 features; thus, we create a 4x2 dimensional matrix of our samples and features.\n",
    "Similarly, we create a 2x3 dimensional weight matrix (one row per feature and one column for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs X:\n",
      " [[ 0.1  0.5]\n",
      " [ 1.1  2.3]\n",
      " [-1.1 -2.3]\n",
      " [-1.5 -2.5]]\n",
      "\n",
      "Weights W:\n",
      " [[ 0.1  0.2  0.3]\n",
      " [ 0.1  0.2  0.3]]\n",
      "\n",
      "bias:\n",
      " [ 0.01  0.1   0.1 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.1, 0.5],\n",
    "              [1.1, 2.3],\n",
    "              [-1.1, -2.3],\n",
    "              [-1.5, -2.5]])\n",
    "\n",
    "W = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "\n",
    "bias = np.array([0.01, 0.1, 0.1])\n",
    "\n",
    "print('Inputs X:\\n', X)\n",
    "print('\\nWeights W:\\n', W)\n",
    "print('\\nbias:\\n', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the net input, we multiply the 4x2 matrix feature matrix `X` with the 2x3 (n_features x n_classes) weight matrix `W`, which yields a 4x3 output matrix (n_samples x n_classes) to which we then add the bias unit: \n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net input:\n",
      " [[ 0.07  0.22  0.28]\n",
      " [ 0.35  0.78  1.12]\n",
      " [-0.33 -0.58 -0.92]\n",
      " [-0.39 -0.7  -1.1 ]]\n"
     ]
    }
   ],
   "source": [
    "def net_input( X, W, b ):\n",
    "    return X.dot(W) + b\n",
    "\n",
    "net_in = net_input( X, W, bias )\n",
    "print('net input:\\n', net_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to compute the softmax activation that we discussed earlier:\n",
    "\n",
    "$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=1}^{k} e^{z_{j}^{(i)}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:\n",
      " [[ 0.29450637  0.34216758  0.36332605]\n",
      " [ 0.21290077  0.32728332  0.45981591]\n",
      " [ 0.42860913  0.33380113  0.23758974]\n",
      " [ 0.44941979  0.32962558  0.22095463]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum( np.exp(z), axis = 1, keepdims = True )\n",
    "\n",
    "smax = softmax(net_in)\n",
    "print('softmax:\\n', smax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values for each sample (row) nicely sum up to 1 now. E.g., we can say that the first sample  `[ 0.29450637  0.34216758  0.36332605]` has a 29.45% probability to belong to class 0. Now, in order to turn these probabilities back into class labels, we could simply take the argmax-index position of each row:\n",
    "\n",
    "[[ 0.29450637  0.34216758  **0.36332605**] -> 2   \n",
    "[ 0.21290077  0.32728332  **0.45981591**]  -> 2  \n",
    "[ **0.42860913**  0.33380113  0.23758974]  -> 0  \n",
    "[ **0.44941979**  0.32962558  0.22095463]] -> 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  [2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax( axis = 1 )\n",
    "\n",
    "print( 'predicted class labels: ', to_classlabel(smax) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our predictions are terribly wrong, since the correct class labels are `[0, 1, 2, 2]`. Now, in order to train our logistic model (e.g., via an optimization algorithm such as gradient descent), we need to define a cost function $J(\\cdot)$ that we want to minimize:\n",
    "\n",
    "$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H( T^{(i)}, O^{(i)} )$$\n",
    "\n",
    "which is the average of all cross-entropies over our $n$ training samples. The cross-entropy  function is defined as\n",
    "\n",
    "$$H( T^{(i)}, O^{(i)} ) = -\\sum_m T^{(i)} \\cdot log(O^{(i)})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $T$ stands for \"target\" (i.e., the *true* class labels) \n",
    "- $O$ stands for output -- the computed *probability* via softmax; **not** the predicted class label.\n",
    "- $\\sum_m$ denotes adding up the difference between the target and the output for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Cost: 1.32159787159\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_cost( output, y_target ):\n",
    "    return np.mean( -np.sum( np.log(output) * y_target, axis = 1 ) )\n",
    "\n",
    "cost = cross_entropy_cost( output = smax, y_target = y_encode )\n",
    "print('Cross Entropy Cost:', cost )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn the weight for our softmax model via gradient descent, we then need to compute the gradient of our cost function for each class $j \\in \\{0, 1, ..., k\\}$.\n",
    "\n",
    "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$\n",
    "\n",
    "We won't be going through the tedious details here, but this cost's gradient turns out to be simply:\n",
    "\n",
    "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum^{n}_{i=0} \\big[\\mathbf{x}^{(i)}_j\\ \\big( O^{(i)} - T^{(i)} \\big) \\big]$$\n",
    "\n",
    "We can then use the cost derivate to update the weights in opposite direction of the cost gradient with learning rate $\\eta$:\n",
    "\n",
    "$$\\mathbf{w}_j := \\mathbf{w}_j - \\eta \\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$ \n",
    "\n",
    "(note that $\\mathbf{w}_j$ is the weight vector for the class $y=j$), and we update the bias units using:\n",
    "\n",
    "$$\n",
    "\\mathbf{b}_j := \\mathbf{b}_j   - \\eta \\bigg[ \\frac{1}{n} \\sum^{n}_{i=0} \\big( O^{(i)} - T^{(i)} \\big) \\bigg]\n",
    "$$ \n",
    " \n",
    "\n",
    "As a penalty against complexity, an approach to reduce the variance of our model and decrease the degree of overfitting by adding additional bias, we can further add a regularization term such as the L2 term with the regularization parameter $\\lambda$:\n",
    "    \n",
    "$$\\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$$\n",
    "\n",
    "where $||\\mathbf{w}||_{2}^{2}$ simply means adding up the squared weights across all the features and classes.\n",
    "\n",
    "$$||\\mathbf{w}||_{2}^{2} = \\sum^{m}_{l=0} \\sum^{k}_{j=0} w_{l, j}^2$$\n",
    "\n",
    "so that our cost function becomes\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H( T^{(i)}, O^{(i)} ) + \\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}\n",
    "$$\n",
    "\n",
    "and we define the \"regularized\" weight update as\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_j := \\mathbf{w}_j -  \\eta \\big[\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}) + \\lambda \\mathbf{w}_j \\big]\n",
    "$$\n",
    "\n",
    "Note that we don't regularize the bias term, thus the update function for it stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression Code\n",
    "\n",
    "Bringing the concepts together, we could come up with an implementation as follows: Note that for the weight and bias parameter, we'll have initialize a value for it. Here we'll simply draw the weights from a normal distribution and set the bias as zero. The code can be obtained [here](https://github.com/ethen8181/machine-learning/blob/master/text_classification/logistic_regression/softmax.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# manually standardize the feautures\n",
    "for i in range(X.shape[1]):\n",
    "    X[ :, i ] = ( X[ :, i ] - X[ :, i ].mean() ) / X[ :, i ].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<softmax.SoftmaxRegression at 0x11601c3c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from softmax import SoftmaxRegression\n",
    "\n",
    "# train the softmax using batch gradient descent\n",
    "softmax_reg = SoftmaxRegression( eta = 0.01, epochs = 10, minibatches = y.shape[0] )\n",
    "softmax_reg.fit( X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAESCAYAAAD5d3KwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H8z8w0MZugRSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11601c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( range( len(softmax_reg.costs) ), softmax_reg.costs )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88666666666666671"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_pred = softmax_reg.predict(X)\n",
    "accuracy = np.sum( y_pred == y ) / y.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92666666666666664"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit( X, y )\n",
    "y_pred = log_reg.predict(X)\n",
    "accuracy_score( y_true = y, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Sebastian Raschka - Softmax Regression](http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/bonus/softmax-regression.ipynb)\n",
    "- [TensorFlow - Softmax Regression](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
